{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0710, 0.0595, 0.0612, 0.0955, 0.0736, 0.0475, 0.0742, 0.0763, 0.0770,\n",
       "         0.0369, 0.0896, 0.0751, 0.0520, 0.0682, 0.0424]),\n",
       " tensor([0.0710, 0.0595, 0.0612, 0.0955, 0.0736, 0.0475, 0.0742, 0.0763, 0.0770,\n",
       "         0.0369, 0.0896, 0.0751, 0.0520, 0.0682, 0.0424]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_naive(x: torch.Tensor):\n",
    "    m = x.max()\n",
    "    f = (x - m).exp()\n",
    "    l = f.sum()\n",
    "    softmax = f / l\n",
    "    return softmax\n",
    "\n",
    "a = torch.rand((15,))\n",
    "torch.softmax(a, dim=0), softmax_naive(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0440, 0.0609, 0.0417, 0.0654, 0.0873, 0.0994, 0.0434, 0.0647, 0.0428,\n",
       "         0.0982, 0.0640, 0.0617, 0.0536, 0.0838, 0.0891]),\n",
       " tensor([0.0440, 0.0609, 0.0417, 0.0654, 0.0873, 0.0994, 0.0434, 0.0647, 0.0428,\n",
       "         0.0982, 0.0640, 0.0617, 0.0536, 0.0838, 0.0891]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def softmax_tiled(x: torch.Tensor, k=3):\n",
    "    n_blocks = math.ceil(x.size()[0] / k)\n",
    "\n",
    "    max_acc = float(\"-inf\")\n",
    "    f_values = torch.zeros((x.size()[0],))\n",
    "    l_values = torch.zeros((n_blocks,))\n",
    "    m_values = torch.zeros((n_blocks,))\n",
    "\n",
    "    for b in range(0, n_blocks):\n",
    "        x_b = x[b * k:b * k + k]\n",
    "\n",
    "        m_b = x_b.max()\n",
    "\n",
    "        f_b = (x_b - m_b).exp()\n",
    "        l_b = f_b.sum()\n",
    "\n",
    "        f_values[b * k:b * k + k] = f_b\n",
    "        l_values[b] = l_b\n",
    "        m_values[b] = m_b\n",
    "\n",
    "        max_acc = max(max_acc, m_b)\n",
    "\n",
    "    m = max_acc\n",
    "    scaling_factors = (m_values - m).exp()\n",
    "    l = (scaling_factors * l_values).sum()\n",
    "\n",
    "    results = torch.zeros((x.size()[0],))\n",
    "\n",
    "    for b in range(0, n_blocks):\n",
    "        scaling_factor = scaling_factors[b]\n",
    "\n",
    "        f_b = f_values[b * k:b * k + k]\n",
    "        res_b = (f_b * scaling_factor) / l\n",
    "\n",
    "        results[b * k:b * k + k] = res_b\n",
    "\n",
    "    return results\n",
    "\n",
    "a = torch.rand((15,))\n",
    "torch.softmax(a, dim=0), softmax_tiled(a, k=3)\n",
    "# softmax_tiled(a).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax_tiled_oneloop(x: torch.Tensor, k=3):\n",
    "    n_blocks = math.ceil(x.size()[0] / k)\n",
    "\n",
    "    max_acc = float(\"-inf\")\n",
    "    f_values = torch.zeros((x.size()[0],))\n",
    "    l_values = torch.zeros((n_blocks,))\n",
    "    m_values = torch.zeros((n_blocks,))\n",
    "\n",
    "    for b in range(0, n_blocks):\n",
    "        x_b = x[b * k:b * k + k]\n",
    "\n",
    "        m_b = x_b.max()\n",
    "\n",
    "        f_b = (x_b - m_b).exp()\n",
    "        l_b = f_b.sum()\n",
    "\n",
    "        f_values[b * k:b * k + k] = f_b\n",
    "        l_values[b] = l_b\n",
    "        m_values[b] = m_b\n",
    "\n",
    "        max_acc = max(max_acc, m_b)\n",
    "\n",
    "    m = max_acc\n",
    "    scaling_factors = (m_values - m).exp()\n",
    "    l = (scaling_factors * l_values).sum()\n",
    "\n",
    "    results = torch.zeros((x.size()[0],))\n",
    "\n",
    "    for b in range(0, n_blocks):\n",
    "        scaling_factor = scaling_factors[b]\n",
    "\n",
    "        f_b = f_values[b * k:b * k + k]\n",
    "        res_b = (f_b * scaling_factor) / l\n",
    "\n",
    "        results[b * k:b * k + k] = res_b\n",
    "\n",
    "    return results\n",
    "\n",
    "a = torch.rand((15,))\n",
    "torch.softmax(a, dim=0), softmax_tiled(a, k=3)\n",
    "# softmax_tiled(a).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.2988e-04, 3.4815e-03, 8.0749e-04, 3.2067e-03, 2.8264e-03, 2.9001e-03,\n",
       "         3.1164e-03, 6.2981e-03, 4.6885e-03, 1.5142e-03],\n",
       "        [3.4915e-03, 1.6627e-01, 1.0068e-02, 8.9618e-02, 3.1834e-02, 5.2418e-02,\n",
       "         5.2547e-02, 2.5292e-01, 7.5699e-02, 1.7230e-02],\n",
       "        [2.9257e-02, 1.0018e-01, 5.5842e-02, 1.1918e-01, 1.8103e-02, 1.7371e-02,\n",
       "         3.8449e-02, 1.8871e-01, 7.7059e-02, 6.8165e-02],\n",
       "        [2.7490e-03, 9.6256e-02, 2.4736e-01, 4.3178e-02, 1.4328e-01, 1.1132e-03,\n",
       "         8.9672e-03, 3.4262e-01, 4.8426e-01, 4.4858e-01],\n",
       "        [1.0496e-01, 4.5636e-01, 6.7951e-01, 9.2824e-01, 5.7093e-01, 2.7699e-01,\n",
       "         3.5616e-01, 8.3383e-02, 7.9569e-01, 1.0648e+00],\n",
       "        [1.0492e-01, 1.4487e-01, 1.9831e-01, 2.5742e-03, 1.7217e-01, 2.8173e-01,\n",
       "         1.5352e-01, 1.9702e-01, 1.7888e-01, 5.0681e-02],\n",
       "        [2.6330e-01, 1.2877e+00, 2.8098e-01, 7.2526e-01, 9.0763e-01, 3.5823e-02,\n",
       "         1.2726e+00, 2.0113e+00, 1.0532e+00, 1.0247e+00],\n",
       "        [2.7644e-04, 1.0646e-03, 1.6960e-03, 2.3404e-03, 1.4046e-03, 6.9016e-04,\n",
       "         8.9785e-04, 3.0444e-04, 1.9999e-03, 2.6445e-03],\n",
       "        [1.3434e-01, 3.9931e-01, 5.2657e-02, 2.1702e-01, 4.0811e-01, 1.7385e-01,\n",
       "         6.0800e-01, 6.7020e-01, 3.4121e-01, 4.8687e-01],\n",
       "        [6.2439e-01, 5.0530e-02, 1.7391e-01, 6.1370e-01, 1.5166e-01, 3.9262e-01,\n",
       "         9.1894e-02, 7.6616e-01, 1.2911e-01, 3.8566e-01],\n",
       "        [8.3606e-01, 2.7707e-01, 6.5490e-01, 4.5552e-01, 6.5762e-01, 1.0618e-01,\n",
       "         3.9960e-01, 1.9107e+00, 6.3611e-01, 1.0553e-01],\n",
       "        [7.5907e-03, 5.0413e-02, 3.5031e-02, 1.7229e-02, 2.2043e-02, 4.5203e-02,\n",
       "         3.5128e-02, 7.0827e-02, 2.2546e-02, 5.0676e-02],\n",
       "        [8.5989e-03, 3.7729e-02, 2.8997e-02, 5.3080e-02, 3.9443e-02, 1.3033e-02,\n",
       "         3.3774e-02, 2.7864e-02, 5.0718e-02, 6.4693e-02],\n",
       "        [1.7986e-02, 2.0604e-01, 3.9348e-01, 1.3649e-01, 2.2730e-01, 1.4510e-01,\n",
       "         1.1566e-01, 6.5567e-01, 9.1280e-01, 6.5389e-01],\n",
       "        [1.1988e-04, 7.5769e-05, 6.2498e-04, 1.5817e-04, 1.5940e-04, 9.7983e-04,\n",
       "         6.4832e-04, 5.4088e-05, 3.0951e-04, 1.0111e-03],\n",
       "        [3.4811e-03, 4.8064e-02, 9.5683e-02, 3.1412e-02, 5.4620e-02, 3.0128e-02,\n",
       "         2.4417e-02, 1.5539e-01, 2.1384e-01, 1.5787e-01],\n",
       "        [1.1513e-03, 4.9333e-03, 9.0414e-04, 2.1980e-03, 1.4513e-03, 4.0002e-04,\n",
       "         7.3883e-04, 6.1636e-03, 7.3698e-04, 9.1618e-04],\n",
       "        [4.5198e-03, 2.0949e-02, 8.1525e-03, 9.8957e-03, 8.2112e-03, 4.0143e-04,\n",
       "         1.2712e-03, 1.4900e-02, 8.4908e-03, 1.1308e-02],\n",
       "        [4.7101e-04, 2.3315e-03, 1.2449e-03, 1.1316e-03, 1.0661e-03, 1.2321e-04,\n",
       "         5.7381e-05, 8.9649e-04, 1.7193e-03, 1.8343e-03],\n",
       "        [3.7137e-03, 1.2105e-03, 2.2516e-02, 4.0663e-03, 6.6385e-03, 2.9367e-02,\n",
       "         1.9236e-02, 4.1067e-03, 2.3793e-03, 3.6582e-02]], dtype=torch.float64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def FlashAttention(Q, K, V, B=10):\n",
    "    N, d = Q.shape\n",
    "\n",
    "    # Initialize O, l, m\n",
    "    O = torch.zeros_like(Q)\n",
    "    l = torch.zeros(N, dtype=Q.dtype)\n",
    "    m = torch.full((N,), float(\"-inf\"), dtype=Q.dtype)\n",
    "\n",
    "    # Determine the number of blocks\n",
    "    Tr = math.ceil(N / B)\n",
    "    Tc = Tr\n",
    "\n",
    "    # For each column block\n",
    "    for j in range(Tc):\n",
    "        # Load Kj, Vj\n",
    "        Kj = K[j*B:(j+1)*B, :]\n",
    "        Vj = V[j*B:(j+1)*B, :]\n",
    "\n",
    "        # For each row block\n",
    "        for i in range(Tr):\n",
    "            # Load Qi, Oi, li, mi\n",
    "            Qi = Q[i*B:(i+1)*B, :]\n",
    "            Oi = O[i*B:(i+1)*B, :]\n",
    "            li = l[i*B:(i+1)*B]\n",
    "            mi = m[i*B:(i+1)*B]\n",
    "\n",
    "            # On chip, compute Sij, ~mij, ~Pij, ~lij\n",
    "            Sij = Qi @ Kj.T\n",
    "            mij = Sij.max(dim=1)[0]\n",
    "            Pij = torch.exp(Sij - mij.unsqueeze(1))\n",
    "            lij = Pij.sum(dim=1)\n",
    "\n",
    "            # On chip, compute mnewi, lnewi\n",
    "            mnewi = torch.max(mi, mij)\n",
    "            lnewi = torch.exp(mi - mnewi) * li + torch.exp(mij - mnewi) * lij\n",
    "\n",
    "            # Write Oi, li, mi back to HBM\n",
    "            Oi = torch.diag(1. / lnewi) @ (torch.diag(torch.exp(mi - mnewi) * li) @ Oi + torch.exp(mij - mnewi) * Pij @ Vj)\n",
    "            O[i*B:(i+1)*B, :] = Oi\n",
    "            l[i*B:(i+1)*B] = lnewi\n",
    "            m[i*B:(i+1)*B] = mnewi\n",
    "\n",
    "    # Return O\n",
    "    return O\n",
    "\n",
    "N, d = 20, 10\n",
    "Q = torch.randn(N, d, dtype=torch.float64)\n",
    "K = torch.randn(N, d, dtype=torch.float64)\n",
    "V = torch.randn(N, d, dtype=torch.float64)\n",
    "\n",
    "torch.abs(torch.softmax(Q @ K.T, dim=-1) @ V - FlashAttention(Q, K, V))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
